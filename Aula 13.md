[[2024-05-15]]

#### Naive Bayes

Faz parte de uma classe de algoritmos chamada de nÃ£o-paramÃ©trica. Perceptron, SVM, MLP: todos sÃ£o algoritmos paramÃ©tricos que dependem da alteraÃ§Ã£o de pesos para aprender. Naive Bayes aprende atravÃ©s do dado, mas nÃ£o faz isso atravÃ©s de parÃ¢metros. Apesar de Ã¡rvores de decisÃ£o nÃ£o tenha pesos, ela Ã© materializada com uma forma determinada - mas tambÃ©m faz parte da classe de algoritmos nÃ£o paramÃ©tricos.

Naive Bayes consiste em duas principais etapas:
1) Estimar uma distribuiÃ§Ã£o
2) InferÃªncia

##### A parte Bayes
Naive Bayes baseia-se no Teorema de Bayes, que fornece uma maneira de atualizar as probabilidades de uma hipÃ³tese Ã  medida que novas evidÃªncias sÃ£o apresentadas. O teorema Ã© expresso pela seguinte fÃ³rmula:

ğ‘ƒ(ğ‘¦âˆ£ğ‘‹)=ğ‘ƒ(ğ‘‹âˆ£ğ‘¦)â‹…ğ‘ƒ(ğ‘¦)ğ‘ƒ(ğ‘‹)P(yâˆ£X)=P(X)P(Xâˆ£y)â‹…P(y)â€‹

Onde:

- ğ‘ƒ(ğ‘¦âˆ£ğ‘‹)P(yâˆ£X) Ã© a probabilidade posterior da classe ğ‘¦y dado o vetor de features ğ‘‹X.
- ğ‘ƒ(ğ‘‹âˆ£ğ‘¦)P(Xâˆ£y) Ã© a probabilidade de observar o vetor de features ğ‘‹X dado que a classe Ã© ğ‘¦y.
- ğ‘ƒ(ğ‘¦)P(y) Ã© a probabilidade prÃ©via da classe ğ‘¦y.
- ğ‘ƒ(ğ‘‹)P(X) Ã© a probabilidade de observar o vetor de features ğ‘‹X.

##### A parte Naive

Assumiremos aqui a independÃªncia condicional - isto Ã©, que fixando um y, terÃ­amos independÃªncia das features (todo o vetor X). Isso facilita bastante os cÃ¡lculos do cÃ¡lculo das probabilidades para estimar uma hipÃ³tese. As features, neste caso, seriam, entÃ£o, independentes entre si: teriam apenas uma relaÃ§Ã£o com y.

p(xi | y, xj) = p(xi | y)
p(xi | y, xj, xk) = p(xi | y) 
p(xi | y, xj, xk, xl) = p(xi | y)

Essa suposiÃ§Ã£o de independÃªncia Ã© o que torna o algoritmo "ingÃªnuo" (naive). Embora na prÃ¡tica as features muitas vezes nÃ£o sejam completamente independentes, o Naive Bayes pode ainda assim ter um desempenho surpreendentemente bom em muitos problemas de classificaÃ§Ã£o devido Ã  sua simplicidade e eficiÃªncia computacional.

--- 

Naive Bayes Ã© interessante quando queremos um modelo que seja muito rÃ¡pido - seja por limitaÃ§Ã£o de tempo ou de poder computacional. AlÃ©m disso, esse algoritmo se comporta bem mesmo com pouquÃ­ssimos dados. Geralmente nÃ£o se fala de *overfitting* quando o assunto Ã© Naive Bayes.

Naive Bayes nÃ£o Ã©, de fato, um algoritmo lazy. O termo "lazy" refere-se a um tipo especÃ­fico de algoritmo de aprendizado, conhecido como "lazy learning" ou "aprendizado preguiÃ§oso", onde a generalizaÃ§Ã£o do modelo Ã© adiada atÃ© que uma nova amostra precise ser classificada. Exemplos tÃ­picos de algoritmos lazy sÃ£o o k-NN (k-Nearest Neighbors), que nÃ£o constroem um modelo explÃ­cito durante a fase de treinamento, mas simplesmente armazenam os dados de treinamento e realizam a generalizaÃ§Ã£o na hora da classificaÃ§Ã£o. Naive Bayes, por outro lado, Ã© considerado um algoritmo "eager" ou "aprendizado ansioso". Isso significa que ele constrÃ³i um modelo de classificaÃ§Ã£o durante a fase de treinamento, antes de receber novos dados para serem classificados.